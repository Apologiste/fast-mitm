\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{graphicx} 
\usepackage{float}

\setlength{\headheight}{14pt} 
\hbadness=11000  
\vbadness=11000  
% chktex-file 44

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage/\pageref{LastPage}}
\fancyhead[L]{Projet UE PPAR}
\fancyhead[R]{M1 Informatique, Sorbonne Université}

\geometry{margin=2.5cm}

\title{\textbf{Direct Meet-in-the-Middle Attack}}
\author{Julian Mouthon, Mario Razafinony }
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    \begin{center}
    Ce rapport présente la parallèlisation d'un code d'attaque \emph{Meet-in-the-Middle} (MitM) en utilisant MPI et OpenMP.\@
    \end{center}
\end{abstract}

\thispagestyle{fancy} 
\tableofcontents

\section{Introduction}\label{sec:introduction}

L'attaque \emph{Meet-in-the-Middle} est une technique qui exploite la possibilité 
de diviser un chiffrement en deux parties indépendantes. 
Le but est de chercher des paires de clés $(k_1, k_2)$ telles que:

\[
\text{Enc}_{k_1}(\text{Enc}_{k_2}(P_1)) = C_1 \quad \text{et} \quad \text{Enc}_{k_1}(P_0) = \text{Dec}_{k_2}(C_0)
\]

où $(P_0, C_0)$ et $(P_1, C_1)$ sont deux paires texte clair-chiffré connues.

\section{Parallèlisation}\label{sec:architecture}

Pour maximiser les performances, nous combinons plusieurs niveaux de parallélisme:\\

\begin{itemize}
    \item \textbf{MPI}: parallélisme multi-processus
    \item \textbf{OpenMP}: parallélisme multi-thread
    \item \textbf{Sharding}: distribution du dictionnaire entre processus
    \item \textbf{Vectorisation}: accélération de l’initialisation des dictionnaires locaux
\end{itemize}


\subsection{Dictionnaire Partagé}

Dans la version séquentielle, toutes les valeurs $f(x)$ étaient stockées dans un
dictionnaire global unique.
Ici, ce dictionnaire est distribué (\emph{shardé}) entre les processus.\\

Le processus propriétaire d’une clé intermédiaire $z$ est déterminé par:
\[
\text{shard}(z) = z \bmod P
\]

où $P$ est le nombre
total de processus.\\

Chaque processus maintient uniquement son dictionnaire local, ce qui permet:
\begin{itemize}
    \item de réduire la mémoire utilisée par processus
    \item d’améliorer la localité mémoire
    \item de paralléliser les insertions et les recherches\\
\end{itemize}

Nous avons ajouté trois fonctions pour gérer le dictionnaire shardé: 

\begin{itemize}
    \item \texttt{shard\_dict\_setup} pour l’initialisation, 
    \item \texttt{shard\_dict\_insert} pour l’insertion, 
    \item \texttt{shard\_probe\_local} pour la recherche locale.

\end{itemize}

\subsection{MPI}

Chaque processus MPI est responsable:
\begin{itemize}
    \item d’une fraction de l’espace de recherche des clés,
    \item d’un \textbf{shard} du dictionnaire global
    \item du traitement local des collisions correspondant à son shard.\\
\end{itemize}

La répartition ne dépend pas d’un processus maître,
ce qui évite les goulots d’étranglement.

\subsubsection{Découpage de l’espace de recherche}

L’espace des clés $\{0,1\}^n$ est réparti entre les processus MPI selon leur rang:
\[
x \equiv \text{rank} \pmod{\text{P}}
\]

Ainsi, chaque processus traite exactement $\frac{2^n}{P}$ clés, où $P$ est le nombre
total de processus. 

Ce découpage permet:
\begin{itemize}
    \item une charge de travail équilibrée,
    \item l’absence de recouvrement entre processus,
    \item aucune synchronisation nécessaire pendant le calcul local.
\end{itemize}


\subsubsection{Phase 1: Construction distribuée du dictionnaire}

Chaque processus calcule $z = f(x)$ pour ses valeurs locales de $x$:
\begin{itemize}
    \item si le processus courant est propriétaire de $z$, l’entrée $(z,x)$ est insérée
    directement dans le dictionnaire local ;
    \item sinon, la paire est stockée dans un buffer temporaire destinée au processus propriétaire.
\end{itemize}

Les communications sont ensuite regroupées:
\begin{itemize}
    \item échange des tailles à l’aide de \texttt{MPI\_Alltoall},
    \item envois et réceptions asynchrones (\texttt{MPI\_Isend} / \texttt{MPI\_Irecv}),
    \item insertion locale après réception.
\end{itemize}

Ceci permet d’éviter un envoi MPI à chaque itération, ce qui aurait été
extrêmement coûteux.

\subsubsection{Phase 2: Recherche distribuée des collisions}

\begin{itemize}
    \item chaque processus calcule $y = g(z)$ pour ses valeurs de $z$,
    \item si le shard correspondant est local, on peut chercher dans le dictionnaire directement,
    \item sinon, le couple $(y,z)$ est envoyé au processus propriétaire.
\end{itemize}

Les collisions candidates sont ensuite validées localement avec le second couple
clair–chiffré $(P_1, C_1)$.

\subsubsection{Phase 3: Agrégation des résultats}

Chaque processus conserve uniquement ses résultats locaux.
À la fin de l’exécution:
\begin{itemize}
    \item le nombre de solutions locales est collecté avec \texttt{MPI\_Gather},
    \item les clés $(k_1, k_2)$ sont rassemblées sur le processus 0 via \texttt{MPI\_Gatherv}.
\end{itemize}

Cette étape est peu coûteuse car le nombre de solutions est très faible comparé
à la taille de l’espace de recherche.


\subsection{OpenMP}


À l’intérieur de chaque processus, le parallélisme est renforcé à l’aide d’OpenMP.
L’objectif est d’exploiter le parallélisme multi-cœur local tout en gardant
une compatibilité avec les communications MPI.

Les boucles principales des phases 1 et 2 sont parallélisées à l’aide de
\texttt{\#pragma omp parallel for} avec un ordonnancement statique. Chaque thread
traite ainsi une partie disjointe des clés locales, sans recouvrement.

Chaque thread dispose de buffers privés pour stocker temporairement 
les paires clé–valeur destinées aux autres processus MPI. 
Ces buffers locaux sont ensuite fusionnés dans des buffers globaux
dans des sections critiques, avant les phases de communication MPI.

Les insertions dans le dictionnaire shardé sont sûres : 
elles sont effectuées soit par un seul thread après la réception MPI, 
soit en parallèle sur des clés distinctes lors de la construction locale. 
De même, chaque thread valide les collisions localement, 
et les résultats sont stockés dans des tableaux partagés protégés 
par des sections critiques pour éviter toute condition de concurrence.

\subsection{Vectorisation}

La vectorisation est utilisée lors de l’initialisation des dictionnaires locaux (\texttt{shard\_dict\_setup}). 
Grâce à la directive \texttt{\#pragma omp simd}, plusieurs entrées du dictionnaire sont initialisées en parallèle, 
ce qui accélère la phase de préparation.

\section{Algorithme principal (version simplifiée)}

\begin{algorithm}[H]
\caption{Algorithme golden\_claw\_search}
\begin{algorithmic}[1]
\Procedure{golden\_claw\_search}{maxres, k1[], k2[]}
    \State \textbf{Phase 1 : Construction du dictionnaire}
    \For{$x$ tel que $x \equiv \text{rank} \pmod{\text{world\_size}}$}
        \State $z \gets f(x)$
        \State $shard \gets z \bmod \text{world\_size}$
        \If{$shard = \text{rank}$}
            \State \Call{shard\_dict\_insert}{$z, x$}
        \Else
            \State Ajouter $(z,x)$ dans un buffer pour les envoyer plus tard $shard$
        \EndIf
    \EndFor
    \State Échanger les buffers entre tous les processus
    
    \State \textbf{Phase 2 : Recherche de collisions}
    \For{$z$ tel que $z \equiv \text{rank} \pmod{\text{world\_size}}$}
        \State $y \gets g(z)$
        \State $shard \gets y \bmod \text{world\_size}$
        \If{$shard = \text{rank}$}
            \State $X \gets \Call{shard\_probe\_local}{y}$
            \For{$x \in X$}
                \If{\Call{is\_good\_pair}{$x, z$}}
                    \State Ajouter $(x,z)$ aux résultats locaux
                \EndIf
            \EndFor
        \Else
            \State Ajouter $(y,z)$ dans un buffer pour les envoyer plus tard $shard$
        \EndIf
    \EndFor
    \State Échanger et traiter les buffers
    
    \State \textbf{Phase 3 : Agrégation des résultats}
    \State Collecter tous les résultats sur le processus 0
    \State \Return solutions trouvées
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Schéma détaillé}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/schema.png}
\end{figure}

\section{Résultats}

\begin{verbatim}
oarsub -l nodes=1/core=64,walltime=01:00:00 \
  "mpiexec --mca pml ob1 --mca btl tcp,self -n 64 \
  a.out --n 36 --C0 2ddbc9fe4da17416 --C1 fab28111310cbea3"
\end{verbatim}

\begin{itemize}
    \item Nombre de processus MPI: \texttt{64}
    \item Nombre de threads OpenMP: \texttt{36}
    \item \texttt{n = 36}
    \item \texttt{C0 = (4da17416, 2ddbc9fe)}
    \item \texttt{C1 = (310cbea3, fab28111)}
    \item Solution trouvée: \texttt{(8c41a4b9b, fcd233151)}
    \item Temps d'éxecution: \texttt{979 secondes (16 minutes)}
\end{itemize}



\section{Difficultés rencontrées}

La parallélisation s’est révélée plus complexe que prévu. 
Avant d’aboutir à la version finale présentée dans ce rapport, 
plusieurs approches ont été explorées et abandonnées en raison de performances insuffisantes.

\subsection{Approche Boss-Workers}

Une première implémentation reposait sur un schéma \emph{boss-workers}:\\
\begin{itemize}
    \item un processus maître distribuait dynamiquement des blocs de clés aux travailleurs,
    \item les travailleurs effectuaient les calculs puis renvoyaient leurs résultats au maître.\\
\end{itemize}

Cette approche s’est révélée être contre-productive pour plusieurs raisons:\\
\begin{itemize}
    \item le processus maître devenait rapidement un \textbf{goulot d’étranglement}.
    \item les communications étaient très fréquentes et de petite taille, ce qui augmentait la latence MPI
    \item la charge de messages et de synchronisations annulaient complètement les bénéfices du parallélisme.\\
\end{itemize}

Le Boss avait besoin de beaucoup de mémoire RAM, et il y a avait de nombreux workers.
Cela créait un scénario de `botttleneck'.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{images/bottleneck.png}
\end{figure}

Cette version parallèle était bien \textbf{plus lente que le code séquentiel}.


\subsection{Communications à chaque itération}

Une deuxième tentative était proche de la version finale, 
mais avec une stratégie de communication différente:\\
\begin{itemize}
    \item chaque paire $(z,x)$ ou $(y,z)$ était envoyée immédiatement à son processus propriétaire à l’aide de \texttt{MPI\_Send},
    \item les communications étaient donc déclenchées à chaque itération de boucle\\
\end{itemize}

Cette version a rapidement montré ses limites:\\
\begin{itemize}
    \item elle générait un \textbf{nombre extrêmement élevé de messages MPI} et de petites tailles.
    \item le coût de gestion des communications dominait le temps de calcul
    \item la saturation du réseau dégradait les performances\\
\end{itemize}

Cette version était aussi bien \textbf{plus lente que le code séquentiel}.


\end{document}